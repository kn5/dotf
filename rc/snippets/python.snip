snippet     importchainer
abbr        import numpy, chainer,: ...
options     head
	import numpy as np
	import pandas as pd
	import chainer
	from chainer import cuda, Function, gradient_check, report, training, utils, Variable
	from chainer import datasets, iterators, optimizers, serializers
	from chainer import Link, Chain, ChainList
	import chainer.functions as F
	import chainer.links as L
	from chainer.training import extensions
	import argparse
	import pickle
	import matplotlib.pyplot as plt

snippet     importrandom
abbr        random.seed(0),: ...
options     head
	random.seed(0)
	np.random.seed(0)
	if chainer.cuda.available:
		chainer.cuda.cupy.random.seed(0)

snippet     classc
abbr        class NN(object): ...
options     head
	class ${1:#:name}(${2:object}):

		def __init__(self, ${3}):
			${4:pass}
			super($1, self).__init__()
			with self.init_scope():
				${5:pass}

		def __call__(self, x):
			${0:pass}
			return h

snippet     df
abbr        df = pd.read_csv(FILENAME) ...
options     head
	df = pd.read_csv("${1:#:file_path}", sep=${0:"\t"})

snippet     parser
abbr        parser = argparse.ArgumentParser() ...
options     head
	parser = argparse.ArgumentParser()
	parser.add_argument('--gpu', dest='gpu', type=int, default=-1)
	parser.add_argument('--traindata', dest='traindata', type=str, default='${1:#:traindata_path}')
	parser.add_argument('--devdata', dest='devdata', type=str, default='${2:#:devdata_path}')
	parser.add_argument('--testdata', dest='testdata', type=str, default='${3:#:testdata_path}')
	parser.add_argument('--batchsize', dest='batchsize', type=int, default='${4:#:batchsize}')
	parser.add_argument('--epoch', dest='epoch', type=int, default='${5:#:epoch}')
	args = parser.parse_args()

	# GPU
	if args.gpu != -1:
		cuda.get_device(args.gpu).use()
		from chainer.cuda import cupy
		xp = cupy
	else:
		xp = np

snippet     deftrain
abbr        def train(network_object, ...
options     head
	def train(network_object, batchsize=128, gpu_id=0, max_epoch=20,
		train_dataset=None, test_dataset=None, postfix='', base=lr=0.01, lr_decay=None)
		# Iterator
		train_iter = iterators.MultiprocessIterator(train, batchsize)
		test_iter = iterators.MultiprocessIterator(test, batchsize, False, False)

		# Model
		net = L.classifier(network_object)

		# Optimizer
		optimizer = optimizers.SGD()
		optimizer.setup(net)

		# Updater
		updater = training.StandardUpdater(train_iter, optimizer, device=gpu_id)

		# Trainer
		trainer = training.Trainer(updater, (max_epoch, 'epoch'), out='result')

		# Trainer extensions
		trainer.extend(extensions.LogReport())
		trainer.extend(extensions.snapshot(filename='snapshot_epoch-{.updater.epoch}'))
		trainer.extend(extensions.Evaluator(test_iter, net, device=gpu_id), name='val')
		trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'val/main/loss', 'val/main/accuracy', 'l1/W/data/std', 'elapsed_time']))
		trainer.extend(extensions.ParameterStatistics(net.predictor.l1, {'std': np.std}))
		trainer.extend(extensions.PlotReport(['l1/W/data/std'], x_key='epoch', file_name='std.png'))
		trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))
		trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))
		trainer.extend(extensions.dump_graph('main/loss'))

		trainer.run()
		del trainer

		return net
